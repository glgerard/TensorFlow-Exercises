{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG implementation in TensorFlow Slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[VGGNet](https://arxiv.org/pdf/1409.1556.pdf) to classify flowers into the 17 categories of the Oxford Flowers data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 32\n",
    "test_size = 0.1\n",
    "pool_size = 2\n",
    "n_maxpool_layers = 5\n",
    "padding = 'VALID'\n",
    "dropout = 0.5\n",
    "display_steps = 2\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "X, Y = oxflower17.load_data(one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_examples, img_height, img_width, img_channels = X.shape\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of training set {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of labels {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = Y.shape[1]\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, axes = plt.subplots(2,2)\n",
    "axes[0,0].imshow(X_train[0])\n",
    "axes[0,1].imshow(X_train[1])\n",
    "axes[1,0].imshow(X_train[2])\n",
    "axes[1,1].imshow(X_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x = tf.placeholder(tf.float32, shape=[None, img_height, img_width, img_channels])\n",
    "#y_ = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
    "#keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Conv 1\n",
    "k_conv1 = 3\n",
    "n_conv1 = 64\n",
    "\n",
    "# Conv 2\n",
    "k_conv2 = 3\n",
    "n_conv2 = 128\n",
    "\n",
    "# Conv 3\n",
    "k_conv3 = 3\n",
    "n_conv3 = 256\n",
    "\n",
    "# Conv 4\n",
    "k_conv4 = 3\n",
    "n_conv4 = 512\n",
    "\n",
    "# Conv 5\n",
    "k_conv5 = 3\n",
    "n_conv5 = 512\n",
    "\n",
    "# FC 6\n",
    "n_fc6 = 4096\n",
    "\n",
    "# FC 7\n",
    "n_fc7 = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg16(x, debug, batch_norm=True):\n",
    "    max_pool_stride=2\n",
    "    wt_init=tf.contrib.layers.xavier_initializer()\n",
    "    net = x\n",
    "    # convolution 1\n",
    "    with tf.name_scope('conv1'):\n",
    "        for _ in range(2):\n",
    "            net = tf.layers.conv2d(net, n_conv1, [k_conv1, k_conv1], padding=padding,\n",
    "                                   activation=tf.nn.relu, kernel_initializer=wt_init)\n",
    "    net = tf.layers.max_pooling2d(net, pool_size, max_pool_stride, padding=padding, name='pool1')\n",
    "    if batch_norm:\n",
    "        net = tf.layers.batch_normalization(net)\n",
    "    if debug:\n",
    "        print('Conv1 shape {}'.format(net.shape))\n",
    "            \n",
    "    # convolution 2\n",
    "    with tf.name_scope('conv2'):\n",
    "        for _ in range(2):\n",
    "            net = tf.layers.conv2d(net, n_conv2, [k_conv2, k_conv2], padding=padding,\n",
    "                                   activation=tf.nn.relu, kernel_initializer=wt_init)\n",
    "    net = tf.layers.max_pooling2d(net, pool_size, max_pool_stride, padding=padding, name='pool2')\n",
    "    if batch_norm:\n",
    "        net = tf.layers.batch_normalization(net)\n",
    "    if debug:\n",
    "        print('Conv2 shape {}'.format(net.shape))\n",
    "\n",
    "    # convolution 3\n",
    "    with tf.name_scope('conv3'):\n",
    "        for _ in range(3):\n",
    "            net = tf.layers.conv2d(net, n_conv3, [k_conv3, k_conv3], padding=padding,\n",
    "                                   activation=tf.nn.relu, kernel_initializer=wt_init)\n",
    "    net = tf.layers.max_pooling2d(net, pool_size, max_pool_stride, padding=padding, name='pool3')\n",
    "    if batch_norm:\n",
    "        net = tf.layers.batch_normalization(net)\n",
    "    if debug:\n",
    "        print('Conv3 shape {}'.format(net.shape))\n",
    "\n",
    "    # convolution 4\n",
    "    with tf.name_scope('conv4'):\n",
    "        for _ in range(3):\n",
    "            net = tf.layers.conv2d(net, n_conv4, [k_conv4, k_conv4], padding=padding,\n",
    "                                   activation=tf.nn.relu, kernel_initializer=wt_init)\n",
    "    net = tf.layers.max_pooling2d(net, pool_size, max_pool_stride, padding=padding, name='pool4')\n",
    "    if batch_norm:\n",
    "        net = tf.layers.batch_normalization(net)\n",
    "    if debug:\n",
    "        print('Conv4 shape {}'.format(net.shape))\n",
    "            \n",
    "    # convolution 5\n",
    "    with tf.name_scope('conv5'):\n",
    "        for _ in range(3):\n",
    "            net = tf.layers.conv2d(net, n_conv5, [k_conv5, k_conv5], padding=padding,\n",
    "                                   activation=tf.nn.relu, kernel_initializer=wt_init)\n",
    "    net = tf.layers.max_pooling2d(net, pool_size, max_pool_stride, padding=padding, name='pool5')\n",
    "    if batch_norm:\n",
    "        net = tf.layers.batch_normalization(net)\n",
    "    if debug:\n",
    "        print('Conv5 shape {}'.format(net.shape))\n",
    "            \n",
    "    # tf.summary.histogram('pool_5', net)\n",
    "        \n",
    "    # fully connected layers\n",
    "    \n",
    "    # Compute dense layer size\n",
    "    if padding == 'SAME':\n",
    "        downsampling_factor = math.pow(pool_size, n_maxpool_layers)\n",
    "        dense_layer_size = int(img_height / downsampling_factor) * int(img_width / downsampling_factor) * n_conv5\n",
    "    else:\n",
    "        dense_layer_size = n_conv5\n",
    "        \n",
    "    net = tf.reshape(net,[-1, dense_layer_size]) # flatten\n",
    "    net = tf.layers.dense(net, n_fc6, name='fc6',\n",
    "                          activation=tf.nn.relu, kernel_initializer=wt_init)\n",
    "    net = tf.nn.dropout(net, 1.0 - dropout, name='dropout6')\n",
    "    net = tf.layers.dense(net, n_fc7, name='fc7',\n",
    "                          activation=tf.nn.relu, kernel_initializer=wt_init)\n",
    "    net = tf.nn.dropout(net, 1.0 - dropout, name='dropout7')\n",
    "    net = tf.layers.dense(net, n_classes, kernel_initializer=wt_init, name='fc8')\n",
    "        \n",
    "    # tf.summary.histogram('scores', net)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\"Model function for Estimator.\"\"\"\n",
    "\n",
    "    # TODO: add tensorboard\n",
    "    \n",
    "    output = vgg16(features[\"x\"], False)\n",
    "\n",
    "    predictions = tf.argmax(output, 1)\n",
    "\n",
    "    # Provide an estimator spec for `ModeKeys.PREDICT`.\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\"classes\": predictions})\n",
    "\n",
    "    # Calculate loss using cross entropy\n",
    "    loss = tf.losses.softmax_cross_entropy(labels, output)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Calculate accuracy as additional eval metric\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(tf.argmax(labels, 1), predictions)\n",
    "    }\n",
    "\n",
    "    # TODO: add accuracy to tensorboard summary\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=params[\"learning_rate\"])\n",
    "    train_op = optimizer.minimize(\n",
    "      loss=loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model params\n",
    "model_params = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "}\n",
    "\n",
    "# Instantiate Estimator\n",
    "nn = tf.estimator.Estimator(model_fn=model_fn, params=model_params)\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train},\n",
    "    y=Y_train,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "\n",
    "# Train\n",
    "nn.train(input_fn=train_input_fn, steps=5000)\n",
    "\n",
    "# Score accuracy\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_test},\n",
    "    y=Y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "ev = nn.evaluate(input_fn=test_input_fn)\n",
    "print(\"Loss: %s\" % ev[\"loss\"])\n",
    "print(\"Accuracy: %s\" % ev[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
